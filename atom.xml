<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ann&#39;s Note</title>
  
  
  <link href="https://annsonic.github.io/atom.xml" rel="self"/>
  
  <link href="https://annsonic.github.io/"/>
  <updated>2022-02-09T10:48:06.264Z</updated>
  <id>https://annsonic.github.io/</id>
  
  <author>
    <name>Ann</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Elastic transform</title>
    <link href="https://annsonic.github.io/2022/07/09/Elastic-transform/"/>
    <id>https://annsonic.github.io/2022/07/09/Elastic-transform/</id>
    <published>2022-07-09T07:01:51.000Z</published>
    <updated>2022-02-09T10:48:06.264Z</updated>
    
    <content type="html"><![CDATA[<h2 id="觀點與事實"><a href="#觀點與事實" class="headerlink" title="觀點與事實"></a>觀點與事實</h2><p>Elastic transform 對於手寫文字和醫學影像是很有用的增量技巧，<br>2003年論文[1]首度提出時應用於MNIST資料集，<br>簡單的CNN模型驗證10,000張影像達到分類準確率99.6%，<br>2015年的Unet[2]時應用於細胞的影像切割，<br>Unet模型驗證ISBI cell tracking challenge的PhC-U373資料集達到平均IoU為0.92。</p><span id="more"></span><h2 id="背景簡介"><a href="#背景簡介" class="headerlink" title="背景簡介"></a>背景簡介</h2><p>隨機地移動像素造成形變，又結合線性轉換來增加變化：<br>(1)<br>線性轉換使用 Affine transformation，<br>它涵蓋 translation 位移、rotation 旋轉、scaling 放大縮小（包含鏡射）、shear 推移這些轉換，<br>線性轉換的特性是會保持兩點之間的比例，<br>所以原先的中心點仍然是中心點。<br>可以參考 [3][4]。<br>(2)<br>像素的移動量是隨機決定，<br>移動量再經過捲積來增加相鄰像素彼此空間上的相關性，<br>隨機移動的像素在畫面上看起來像雜訊，<br>因此需要降低隨機雜訊的平滑化濾波器，<br>在此捲積 kernel 採 Gaussian filter[5]。</p><h2 id="方法與實驗結果"><a href="#方法與實驗結果" class="headerlink" title="方法與實驗結果"></a>方法與實驗結果</h2><p>我讀的是 albumentations 的程式碼[6]。</p><p>它先進行 Affine transformation，<br>選定的座標點是圖片左上角、左下角和右下角，<br>距離邊界 1&#x2F;6 邊長的3個點，<br>所以變數 <code>alpha_affine</code> 值應該要小於圖片尺寸的 1&#x2F;6，<br>超過的話影像會發生鏡射。<br><img src="/2022/07/09/Elastic-transform/affine.png" alt="reflection" title="Reflection by affine"></p><p>移動像素的部份，<br>移動量初始時值域為 [-1, 1]，<br>與 Gaussian filter 做捲積，<br>如果啟用變數 <code>approximate</code> 的話，<br>捲積的 kernel 大小固定為 17*17，<br>套用 OpenCV 的 <code>GaussianBlur</code> 函式，<br>計算速度快，<br>反之則計算 kernel 大小，<br>套用 scipy 的 <code>gaussian_filter</code> 函式，<br>kernel 大小 w 的計算公式<br><code>w = 2 * int(truncate * sigma + 0.5) + 1</code>，<br>truncate 預設為 4。</p><p>我們可以調整變數 <code>sigma</code>，<br>即分佈的標準差，<br>若標準差小（例如sigma &#x3D; 0.01），<br>捲積後位移向量仍然呈現隨機狀，<br>所以形變的影像彷彿加上很多雜訊，<br>看起來很粗糙；<br>若標準差適當（例如sigma &#x3D; 4），<br>捲積後相鄰的位移向量會呈現方向上的漸變，<br>這樣可以得到彈性形變的效果，<br>若標準差太大（例如sigma &#x3D; 16），<br>位移向量的變化變得平均，<br>彈性形變的扭曲效果便不明顯。<br>又，因為高斯分佈的峰值會隨標準差變大而下降，<br>需要搭配倍率來增幅，<br>倍率變數 <code>alpha</code>。 </p><p><img src="/2022/07/09/Elastic-transform/1.png" alt="Small sigma" title="Elastic without affine, with sigma = 0.01 and alpha = 5, 34"><br><img src="/2022/07/09/Elastic-transform/2.png" alt="Small sigma 2" title="Elastic without affine, with sigma = 1 and alpha = 34"><br><img src="/2022/07/09/Elastic-transform/3.png" alt="Medium sigma" title="Elastic without affine, with sigma = 4 and alpha = 34"><br><img src="/2022/07/09/Elastic-transform/4.png" alt="Large sigma" title="Elastic without affine, with sigma = 16 and alpha = 34"></p><h2 id="程式碼"><a href="#程式碼" class="headerlink" title="程式碼"></a>程式碼</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import random</span><br><span class="line">import cv2</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Magic numbers</span><br><span class="line">alpha_affine = 112 // 6 # Unit: pixels</span><br><span class="line">sigma = 4 # Unit: pixels</span><br><span class="line">alpha = 34</span><br><span class="line">border_mode = cv2.BORDER_REFLECT_101</span><br><span class="line"></span><br><span class="line"># Read the source image</span><br><span class="line">src = cv2.imread(&#x27;6.png&#x27;)</span><br><span class="line">colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]</span><br><span class="line"></span><br><span class="line"># Select 3 points (array pts1) for transformation</span><br><span class="line">height, width = src.shape[:2]</span><br><span class="line">center_square = np.float32((height, width)) // 2</span><br><span class="line">square_size = min((height, width)) // 3</span><br><span class="line">pts1 = np.float32(</span><br><span class="line">        [</span><br><span class="line">            center_square + square_size,</span><br><span class="line">            [center_square[0] + square_size, center_square[1] - square_size],</span><br><span class="line">            center_square - square_size,</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"># Visualize and save image</span><br><span class="line">src_pts1 = src.copy()</span><br><span class="line">for i,pt in enumerate(pts1):</span><br><span class="line">    cy, cx = int(pt[0]), int(pt[1])</span><br><span class="line">    cv2.circle(src_pts1, (cx,cy), radius=1, color=colors[i], thickness=-1)</span><br><span class="line">cv2.imwrite(&#x27;pts1.png&#x27;, src_pts1)</span><br><span class="line"></span><br><span class="line"># Perturb 3 points to random positions</span><br><span class="line">random_state = np.random.RandomState(random.randint(0, (1 &lt;&lt; 32) - 1))</span><br><span class="line">pts2 = pts1 + random_state.uniform(low=-alpha_affine, high=alpha_affine, size=pts1.shape).astype(</span><br><span class="line">        np.float32)</span><br><span class="line"># Visualize and save image</span><br><span class="line">src_pts2 = src.copy()</span><br><span class="line">for i,pt in enumerate(pts2):</span><br><span class="line">    cy, cx = int(pt[0]), int(pt[1])</span><br><span class="line">    cv2.circle(src_pts2, (cx,cy), radius=1, color=colors[i], thickness=-1)</span><br><span class="line">cv2.imwrite(&#x27;pts2.png&#x27;, src_pts2)</span><br><span class="line"></span><br><span class="line"># Compute the affine transform matrix</span><br><span class="line">matrix = cv2.getAffineTransform(pts1, pts2)</span><br><span class="line"></span><br><span class="line"># Apply transformation</span><br><span class="line">dst_affine = cv2.warpAffine(src_pts2, matrix, (height,width))</span><br><span class="line"># Visualize and save image</span><br><span class="line">cv2.imwrite(&#x27;affine.png&#x27;, dst_affine)</span><br><span class="line"></span><br><span class="line"># Generate random displacement field in x-direction, range [-1, 1]</span><br><span class="line">dx = random_state.rand(height, width).astype(np.float32) * 2 - 1</span><br><span class="line"># Apply smoothing</span><br><span class="line">cv2.GaussianBlur(dx, (17, 17), sigma, dst=dx, borderType=border_mode)</span><br><span class="line">dx *= alpha</span><br><span class="line"># Duplicate random vector field in y-direction</span><br><span class="line">dy = dx</span><br><span class="line"># Position indices x, y</span><br><span class="line">x, y = np.meshgrid(np.arange(width), np.arange(height))</span><br><span class="line"># Purturbed position indices</span><br><span class="line">map_x = np.float32(x + dx)</span><br><span class="line">map_y = np.float32(y + dy)</span><br><span class="line"># Remap the pixels</span><br><span class="line">dst_mapped = cv2.remap(dst_affine, map_x, map_y, cv2.INTER_LINEAR)</span><br><span class="line"></span><br><span class="line"># Visualize the elastic transformation</span><br><span class="line">fig,ax = plt.subplots(1,1)</span><br><span class="line">ax.imshow(dst_mapped)</span><br><span class="line"># Prune the index array to reduce the arrow density</span><br><span class="line">skip=(slice(None,None,4),slice(None,None,4))</span><br><span class="line">ax.quiver(x[skip], y[skip], dx[skip], dy[skip], </span><br><span class="line">          color=&#x27;orange&#x27;, linewidths=(5,), headaxislength=10)</span><br><span class="line">ax.set_axis_off()</span><br><span class="line">plt.savefig(&#x27;dst.png&#x27;)</span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line"># Initiate the array as the discrete Dirac delta function</span><br><span class="line">gaussian_kernel = np.zeros((17, 17))</span><br><span class="line">gaussian_kernel[8, 8] = 1</span><br><span class="line"># Apply smoothing</span><br><span class="line">cv2.GaussianBlur(gaussian_kernel, (17, 17), sigma, dst=gaussian_kernel, borderType=border_mode)</span><br><span class="line">gaussian_kernel = gaussian_kernel / gaussian_kernel.sum()</span><br><span class="line"># Visualize the kernel and save image</span><br><span class="line">plt.imshow(gaussian_kernel, cmap=plt.get_cmap(&#x27;jet&#x27;), interpolation=&#x27;nearest&#x27;)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.savefig(&#x27;kernel.png&#x27;)</span><br></pre></td></tr></table></figure><h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><p>[1] Simard, Steinkraus and Platt, “Best Practices for Convolutional Neural Networks applied to Visual Document Analysis”, in Proc. of the International Conference on Document Analysis and Recognition, 2003.<br>[2] Ronneberger, Olaf et al. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” MICCAI (2015).<br>[3] Affine transform 矩陣 <a href="https://en.wikipedia.org/wiki/Affine_transformation">Wiki</a><br>[4] Affine Transformation 三點轉換 <a href="https://theailearner.com/tag/cv2-warpaffine/">解釋</a><br>[5] Gaussian filter <a href="https://medium.com/@bob800530/python-gaussian-filter-%E6%A6%82%E5%BF%B5%E8%88%87%E5%AF%A6%E4%BD%9C-676aac52ea17">解釋</a><br>[6] albumentations 的<a href="https://albumentations.ai/docs/api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.ElasticTransform">程式碼</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;觀點與事實&quot;&gt;&lt;a href=&quot;#觀點與事實&quot; class=&quot;headerlink&quot; title=&quot;觀點與事實&quot;&gt;&lt;/a&gt;觀點與事實&lt;/h2&gt;&lt;p&gt;Elastic transform 對於手寫文字和醫學影像是很有用的增量技巧，&lt;br&gt;2003年論文[1]首度提出時應用於MNIST資料集，&lt;br&gt;簡單的CNN模型驗證10,000張影像達到分類準確率99.6%，&lt;br&gt;2015年的Unet[2]時應用於細胞的影像切割，&lt;br&gt;Unet模型驗證ISBI cell tracking challenge的PhC-U373資料集達到平均IoU為0.92。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Interested Paper" scheme="https://annsonic.github.io/tags/Interested-Paper/"/>
    
  </entry>
  
  <entry>
    <title>造型氣球—新娘捧花</title>
    <link href="https://annsonic.github.io/2021/05/16/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83%E2%80%94%E6%96%B0%E5%A8%98%E6%8D%A7%E8%8A%B1/"/>
    <id>https://annsonic.github.io/2021/05/16/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83%E2%80%94%E6%96%B0%E5%A8%98%E6%8D%A7%E8%8A%B1/</id>
    <published>2021-05-16T06:22:23.000Z</published>
    <updated>2022-02-08T04:05:37.744Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/05/16/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83%E2%80%94%E6%96%B0%E5%A8%98%E6%8D%A7%E8%8A%B1/bouquet.jpg" alt="flower bouquet" title="balloon twisting bouquet"></p><span id="more"></span><p>參考Changsunny小姐的教學做的~ <a href="https://www.youtube.com/watch?v=X56N_Om4uXo">教學影片傳送門</a><br>材料:</p><ul><li>緞帶1：翠綠色160氣球，充氣至尾端留2指</li><li>緞帶2：水綠色160氣球 1&#x2F;3條即可，充氣至尾端留2指 </li><li>花莖：綠色260氣球，充氣至尾端留8指</li><li>花1：粉紅色260氣球 2條，充氣至尾端留8指</li><li>花2：黃&#x2F;紫色260氣球, 充氣至尾端留8指</li><li>剪刀</li><li>黑色奇異筆</li><li>保鮮膜的軸心棒</li></ul><p>Changsunny小姐真的很有天份，<br>很多獨創的造型，<br>但頻道很久沒更新了，希望她一切都好。</p><p>朋友要結婚了，<br>因為疫情沒有辦公開婚禮，<br>所以做個花束拍起來傳達祝福，<br>緞帶的製作，如果能像影片那樣利用棒子先將氣球纏繞起來再吹氣，<br>相信緞帶的捲度會更好看。</p><p>因為我自己容易倒帶之後反而忘記我目前做到哪一個步驟,<br>所以參考台大氣球社一位學長提出的記譜方式製作這張流程圖<br><img src="/2021/05/16/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83%E2%80%94%E6%96%B0%E5%A8%98%E6%8D%A7%E8%8A%B1/g.jpg" alt="flow" title="balloon twisting flow"><br>氣球譜 <a href="https://mropengate.blogspot.com/2016/03/blog-post_87.html">原文</a><br>邊上的文字是表示 – 第幾個步驟:氣泡長度(單位是自己的手指頭寬度）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2021/05/16/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83%E2%80%94%E6%96%B0%E5%A8%98%E6%8D%A7%E8%8A%B1/bouquet.jpg&quot; alt=&quot;flower bouquet&quot; title=&quot;balloon twisting bouquet&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Balloon Twisting" scheme="https://annsonic.github.io/categories/Balloon-Twisting/"/>
    
    
    <category term="wedding bouquet" scheme="https://annsonic.github.io/tags/wedding-bouquet/"/>
    
  </entry>
  
  <entry>
    <title>造型氣球—Elmo</title>
    <link href="https://annsonic.github.io/2021/05/09/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83%E2%80%94Elmo/"/>
    <id>https://annsonic.github.io/2021/05/09/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83%E2%80%94Elmo/</id>
    <published>2021-05-09T06:13:44.000Z</published>
    <updated>2022-02-08T03:50:44.603Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/05/09/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83%E2%80%94Elmo/elmo.jpg" alt="Elmo" title="balloon twisting Elmo"></p><span id="more"></span><p>我忘記看哪個教學做的，如果有想起再補上。<br>材料:</p><ul><li>身體：紅色260氣球3條, 充氣至尾端留6指</li><li>眼睛：白色5”氣球, 充氣至掌心大</li><li>鼻子：粉&#x2F;橙色5”氣球, 比眼睛大一些</li><li>剪刀</li><li>黑色奇異筆</li></ul><p>小時候有在芝麻街補習過，<br>老早忘記故事人物關係了，<br>卻對Elmo的大紅色和笑容印象深刻，<br>做一隻Elmo來謝謝媽媽花錢讓我學英文。<br>Elmo 真的是配色對了，做起來便很像。</p><p>因為我自己容易倒帶之後反而忘記我目前做到哪一個步驟,<br>所以參考台大氣球社一位學長提出的記譜方式製作這張流程圖<br><img src="/2021/05/09/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83%E2%80%94Elmo/g.jpg" alt="flow" title="balloon twisting flow"><br>氣球譜 <a href="https://mropengate.blogspot.com/2016/03/blog-post_87.html">原文</a><br>邊上的文字是表示 – 第幾個步驟:氣泡長度(單位是自己的手指頭寬度）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2021/05/09/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83%E2%80%94Elmo/elmo.jpg&quot; alt=&quot;Elmo&quot; title=&quot;balloon twisting Elmo&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Balloon Twisting" scheme="https://annsonic.github.io/categories/Balloon-Twisting/"/>
    
    
    <category term="Elmo" scheme="https://annsonic.github.io/tags/Elmo/"/>
    
  </entry>
  
  <entry>
    <title>造型氣球-春</title>
    <link href="https://annsonic.github.io/2021/02/15/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E6%98%A5/"/>
    <id>https://annsonic.github.io/2021/02/15/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E6%98%A5/</id>
    <published>2021-02-15T09:04:23.000Z</published>
    <updated>2021-02-15T09:12:33.908Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/02/15/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E6%98%A5/spring.jpg" alt="spring" title="balloon twisting Chinese new year"></p><span id="more"></span><p>依照泡泡來了的教學做的~ <a href="https://www.youtube.com/watch?v=XlHqyr2pjj0">教學影片傳送門</a><br>材料:</p><ul><li>花瓣:白色260氣球，充氣至尾端留5指，半條長度即可</li><li>花蕊:任意顏色，用剩料即可</li><li>春:紅色260氣球，3條，充氣至尾端留8指</li><li>剪刀</li></ul><p>泡泡來了的教法是從最長的筆劃開始做，<br>讓我不會擔心做到後來發生氣球長度不夠用，<br>萬一氣球不夠長了，<br>也只要在中間豎的那一劃上接上新的氣球即可。<br>我媽說這個春字好像人張開雙手，呵…</p><p>因為我自己容易倒帶之後反而忘記我目前做到哪一個步驟,<br>所以參考台大氣球社一位學長提出的記譜方式製作這張流程圖<br><img src="/2021/02/15/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E6%98%A5/g.jpg" alt="flow" title="balloon twisting flow"><br>氣球譜 <a href="https://mropengate.blogspot.com/2016/03/blog-post_87.html">原文</a><br>邊上的文字是表示 – 第幾個步驟:氣泡長度(單位是自己的手指頭寬度）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2021/02/15/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E6%98%A5/spring.jpg&quot; alt=&quot;spring&quot; title=&quot;balloon twisting Chinese new year&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Balloon Twisting" scheme="https://annsonic.github.io/categories/Balloon-Twisting/"/>
    
    
    <category term="Chinese character - Spring" scheme="https://annsonic.github.io/tags/Chinese-character-Spring/"/>
    
  </entry>
  
  <entry>
    <title>計數血液細胞-使用RetinaNet</title>
    <link href="https://annsonic.github.io/2020/11/08/%E8%A8%88%E6%95%B8%E8%A1%80%E6%B6%B2%E7%B4%B0%E8%83%9E-%E4%BD%BF%E7%94%A8RetinaNet/"/>
    <id>https://annsonic.github.io/2020/11/08/%E8%A8%88%E6%95%B8%E8%A1%80%E6%B6%B2%E7%B4%B0%E8%83%9E-%E4%BD%BF%E7%94%A8RetinaNet/</id>
    <published>2020-11-08T13:35:39.000Z</published>
    <updated>2020-11-30T12:34:59.814Z</updated>
    
    <content type="html"><![CDATA[<p>個人練習，以RetinaNet[1]數血液細胞的個數。</p><span id="more"></span><h2 id="主題："><a href="#主題：" class="headerlink" title="主題："></a>主題：</h2><p>計算一張血塗片影像中出現的血小板(Platelet)、紅血球(RBC)和白血球(WBC)的個數。<br>並運用一些訓練技巧: </p><ul><li>增加一個 anchor scale，全部scale為[0.79, 1.0, 1.26, 1.59]</li><li>Stochastic Gradient Descent with Warm Restarts 來調整學習率，參數T_0&#x3D;139(因為資料筆數278), T_mult&#x3D;1, eta_min&#x3D;1e-6<img src="cosine_lr.png" style="width: 50%; height: 50%" title="learning rate schedule"/></li><li>資料增量： Mosaic[2]，參數probability of utilizing Mosaic &#x3D; 50%<img src="mosaic_sample_3.png" style="width: 50%; height: 50%" title="Sample of Mosaic data augmentation"/></li><li>正規化：GridMask[3], DropBlock[4]<br>GridMask，參數 rotation&#x3D;45, mask ratio &#x3D; 80%, probability of utilizing GridMask &#x3D; 50%，程序在做了Mosaic資料增量之後<img src="gridmask_sample.png" style="width: 50%; height: 50%" title="Sample of GridMask regularization"/>DropBlock，實驗顯示套用在PyramidFeatures的P5_1層效果比P3_1層來得好，參數block_size=2, start prob=0, stop probe=0.1, nr_steps=4e3, start_step=0</li></ul><h2 id="資料集："><a href="#資料集：" class="headerlink" title="資料集："></a>資料集：</h2><p>與 baseline (HOG+SVM) <a href="https://annsonic.github.io/2020/10/30/%E8%A8%88%E6%95%B8%E8%A1%80%E6%B6%B2%E7%B4%B0%E8%83%9E-%E4%BD%BF%E7%94%A8HOG%E5%92%8CSVM/#%E8%B3%87%E6%96%99%E9%9B%86">link</a> 相同</p><h2 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h2><ul><li>backbone選用ResNet-18，確認其物件偵測表現遜於backbone為ResNet-50(以確認模型表現還有成長的空間)，然後測試上述訓練技巧，能否提昇backbone為ResNet-18的表現。</li><li>每個方法重複實驗3次，validation數據取平均，取validation mAP最高的模型測試testing資料集。</li><li>圖檔大小與COCO dataset相近，所以維持使用原RetinaNet架構，統一訓練30 epoch。</li></ul><h2 id="結果"><a href="#結果" class="headerlink" title="結果:"></a>結果:</h2><p>   使用 VOC-style Average Precision算法[6]。</p><ul><li>最好的模型的表現：：</li></ul>   <font size="2">   <table>    <tr>      <td></td>      <td></td>      <td colspan="3">Average Precision (avg val/test)(IoU threshold=0.5)</td>      <td></td>    </tr>    <tr>      <td></td>      <td>Method</td>      <td>Platelet</td>      <td>RBC</td>      <td>WBC</td>      <td>mAP (avg val/test)</td>    </tr>      <td>(1)</td>      <td>RetinaNet(ResNet-50)</td>      <td>85.54%±6.40% / 90.59%</td>      <td>81.18%±3.21% / 86.29%</td>      <td>100% / 100%</td>      <td>88.91%±1.25% / 92.29%</td>    </tr>    <tr>      <td>(2)</td>      <td>RetinaNet(ResNet-18)</td>      <td>79.09%±3.28% / 87.00%</td>      <td>77.67%±2.69% / 86.22%</td>      <td>100% / 100%</td>      <td>85.59%±0.73% / 91.07%</td>    </tr>    <tr>      <td>(3)</td>      <td>RetinaNet(ResNet-18)<br> + anchor scale + SGDR</td>      <td>80.18%±2.23% / 98.10%</td>      <td>82.78%±1.76% / 86.63%</td>      <td>100% / 100%</td>      <td>87.65%±0.22% / 94.91%</td>    </tr>    <tr>      <td>(4)</td>      <td>RetinaNet(ResNet-18)<br> + anchor scale + SGDR<br> + GridMask</td>      <td>83.10%±2.06% / 94.33%</td>      <td>80.18%±0.8% / 85.64%</td>      <td>100% / 100%</td>      <td>87.76%±0.58% / 93.32%</td>    </tr>    <tr>      <td>(5)</td>      <td>RetinaNet(ResNet-18)<br> + anchor scale + SGDR<br> + GridMask + Mosaic</td>      <td>85.46%±0.35% / 95.80%</td>      <td>81.22%±1.34% / 87.57%</td>      <td>100% / 100%</td>      <td>88.89%±0.34% / 94.46%</td>    </tr>    <tr>      <td>(6)</td>      <td>RetinaNet(ResNet-18)<br> + anchor scale + SGDR<br> + DropBlock(P3_1)</td>      <td>80.78%±0.49% / NA</td>      <td>81.80%±0.80% / NA</td>      <td>100% / NA</td>      <td>87.53%±0.20% / NA</td>    </tr>    <tr>      <td>(7)</td>      <td>RetinaNet(ResNet-18)<br> + anchor scale + SGDR<br> + DropBlock(P5_1)</td>      <td>83.95%±1.94% / 97.30%</td>      <td>82.17%±1.81% / 85.77%</td>      <td>100% / 100%</td>      <td>88.71%±0.19% / 94.36%</td>    </tr>    <tr>      <td>(8)</td>      <td>RetinaNet(ResNet-18)<br> + anchor scale + SGDR<br> + GridMask + Mosaic<br> + DropBlock(P5_1)</td>      <td>87.05%±6.33% / 98.97%</td>      <td>81.88%±4.48% / 90.59%</td>      <td>100% / 100%</td>      <td>89.64%±1.22% / 96.52%</td>    </tr>   </table>   </font><ul><li><p>precision-recall curve on testing dataset:</p><ul><li>RetinaNet(ResNet-18) + anchor scale + SGDR + GridMask + Mosaic + DropBlock(P5_1)</li></ul> <img src="mAP.png" style="width: 100%; height: 100%" title="Testing mAP trained by SGDR Mosaic GridMask DropBlock"/><p> 得到的數據接近這個部落格(<a href="https://medium.com/@sadeghm/performance-analysis-of-a-cnn-object-detector-for-blood-cell-detection-and-counting-908f9b038f2b">link</a>)的結果。</p></li><li><p>分析bounding box</p> <img src="viz_prediction.png" style="width: 100%; height: 100%" title="Prediction trained by SGDR Mosaic GridMask DropBlock"/><ul><li>最佳模型判斷出不少false positive(如上圖，綠色框為預測，黑色框為ground truth，紅色箭頭指出false positive)，其實也顯得ground truth標得不那麼好，這應該是訓練上的瓶頸。</li><li>在畫面邊緣的紅血球會有沒被偵測的時候，但觀察不出發生的規則，待解惑。</li></ul></li></ul><h2 id="溫故知新"><a href="#溫故知新" class="headerlink" title="溫故知新"></a>溫故知新</h2><ol><li><p>image normalozation的mean和std設定值是否很重要？</p><p>需要做image normalozation的原因跟做batch normalization的原因相似，batch normalization：將輸入資料調整為高斯分佈、降低Internal Covariate Shift問題[5]，增進模型的學習效率。image normalozation的常見算法是對training dataset的每個channel獨立計算mean和variance，用BN取代image normalozation不是個有效率的作法，因為BN的mean和variance是模型逐次學習出來的。</p><p>我使用的RetinaNet其image normalozation的mean和std設定值是由COCO資料集算出來的，比較用Blood Cells資料集算出來的mean和std[6]，雖然肉眼看到的色調明暗差異大，但兩者模型準確度是差異不大。</p></li><li><p>為何凍結BN層參數？</p><p>作者說雖然因為pretrained資料集是Imagenet，與我們的dataset統計分佈有差異，可是訓練物件偵測模型的batch size很小(例如8)，又，有人實驗過，若藉由finetuning來學習新的BN，模型表現反而會像從頭訓練那般，難怪作者會選擇凍結Resnet的BN層參數。[7]</p></li><li><p>使用Focal loss時，model新增的層的初始值要選用zero-weight、constant-bias。</p><p>我實驗如果改用Kaming initialization，訓練初期的的loss會非常大(如下)，我覺得若訓練在別的資料集有可能導致訓練不收斂。</p><font style="font-size:10px;">   Epoch: 0 | Iteration: 0 | Classification loss: 3037.13330 | Regression loss: 9.85530 | Running loss: 3046.98853 <br>   Epoch: 0 | Iteration: 30 | Classification loss: 84.43515 | Regression loss: 1.49457 | Running loss: 764.80437 <br>   Epoch: 0 | Iteration: 60 | Classification loss: 1.43819 | Regression loss: 0.88586 | Running loss: 397.02907 <br>   Epoch: 0 | Iteration: 90 | Classification loss: 0.68479 | Regression loss: 0.87473 | Running loss: 266.69952 <br>   Epoch: 0 | Iteration: 120 | Classification loss: 0.47948 | Regression loss: 0.77096 | Running loss: 200.89332 <br></font></li><li><p>增加小尺寸的anchor在此效果不大，因為RetinaNet最小的anchor是32x32px，偵測物件的條件是IOU &gt; 0.5，sqrt(0.5<em>32</em>32) &#x3D; 22.6 pixels，等效上大小22.6x22.6px 的物件即可被偵測到，而小型血小板的尺寸也約莫20x20px、數量很少；但在使用Mosaic資料增量時，因為4張圖組成一張新圖，物件被縮小為1&#x2F;4倍，便需要更小尺寸的anchor。</p><p>雖然FPN使用C2應該能幫助偵測小物件，但付出的代價是更多anchor、更難訓練好模型[8]</p></li><li><p>SGDR<br>Adam演算法的缺點是容易收斂在局部最佳點，訓練物件偵測模型的話建議使用SGD演算法[9]，而SGDR據說可以更能找到全局最佳點，我實驗學習率的變化週期是0.5 epoch、1 epoch的表現差不多，2 epoch則略遜，感覺它還是靠運氣，找到的血小板AP、紅血球AP最佳點差異不小，也影響從數據比較GridMask、Mosaic、DropBlock的效果。RetinaNet(ResNet-18) + anchor scale + SGDR的3次訓練表現如下表</p><font size="2"><table> <tr>   <td></td>   <td>Platelet</td>   <td>RBC</td>   <td>WBC</td>   <td>validation mAP</td> </tr> <tr>   <td>(1)</td>   <td>77.64%</td>   <td>84.63%</td>   <td>100%</td>   <td>87.43%</td> </tr> <tr>   <td>(2)</td>   <td>81.84%</td>   <td>81.14%</td>   <td>100%</td>   <td>87.66%</td> </tr> <tr>   <td>(3)</td>   <td>81.05%</td>   <td>82.56%</td>   <td>100%</td>   <td>87.87%</td> </tr></table></font></li><li><p>觀察訓練過程中的precision數值，感覺血小板和紅血球的precision有互相消長的傾向，我初期以為是因為血小板的形狀簡單、算是eazy sample，所以實驗加重Focal loss的參數alpha和gamma[10]，但實驗得到血小板的precision數值在訓練的早期有提昇，最終數值卻沒有突破，紅血球表現持平。</p><p>從validation dataset影像檢視血小板的錯誤情形，推測血小板和紅血球的AP互相消長的原因可能是紅血球的陰影、出現在紅血球附近類似氣泡的東西干擾模型的判斷。Mosaic、GridMask和DropBlock技巧有助於提昇血小板的AP，只是得延長訓練時間，原先基本模型大約20 epoch可收斂，加上上述技巧，模型最佳表現出現在20 epoch~30 epoch之間。</p></li><li><p>一開始我疏忽了，使用程式中預設的IoU threshold為0.05，怎麼實驗每個技巧得到的數據都差不多，改為0.5之後便得到顯著差別了。</p></li><li><p>現在我還不知道如何視覺化看到DropBlock在activation上的樣子；要在哪裡加上DropBlock需要盲目嘗試。</p></li><li><p>True positive的計算方式，基本上以IoU為準，與ground truth重疊達一定閥值才算，當有多個預測框同樣框在一個ground truth身上時，預測框會以信心分數高者優先為true positive，其他預測框列為false positive。當有一個預測框同時框了多個ground truth，只能歸屬於當中先被讀取的一個ground truth，其他ground truth算是沒被偵測到。[11]</p></li></ol><h2 id="參考資料："><a href="#參考資料：" class="headerlink" title="參考資料："></a>參考資料：</h2><p> [1] RetinaNet source project <a href="https://github.com/yhenon/pytorch-retinanet">link</a><br> [2] Mosaic <a href="https://github.com/ultralytics/yolov5/blob/master/utils/datasets.py">link</a><br> [3] GridMask source project <a href="https://github.com/zzl-pointcloud/Data_Augmentation_Zoo_for_Object_Detection">link</a><br> [4] DropBlock source project <a href="https://github.com/Randl/MobileNetV3-pytorch/blob/5b69e27c29c99e4c15097045ad4e1857fb928125/MobileNetV3.py">link</a><br> [5] Internal Covariate Shift <a href="https://machinelearning.wtf/terms/internal-covariate-shift/">link</a><br> [6] NVidia的影像正規化介紹 <a href="https://docs.nvidia.com/deeplearning/dali/master-user-guide/docs/examples/general/normalize.html">link</a><br> [7] 凍結BN層參數 <a href="https://stackoverflow.com/questions/63016740/why-its-necessary-to-frozen-all-inner-state-of-a-batch-normalization-layer-when">link</a><br> [8] 不建議FPN使用C2 <a href="https://github.com/fizyr/keras-retinanet/issues/202">link</a><br> [9] 訓練物件偵測模型的話建議使用SGD演算法 <a href="https://github.com/fizyr/keras-retinanet/issues/540">link</a><br>[10] Focal loss的alpha、gamma意義<a href="https://github.com/fizyr/keras-retinanet/issues/710">link</a><br>[11] 物件偵測 True positive的計算方式<a href="https://github.com/rafaelpadilla/Object-Detection-Metrics/issues/46">link</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;個人練習，以RetinaNet[1]數血液細胞的個數。&lt;/p&gt;</summary>
    
    
    
    <category term="Project" scheme="https://annsonic.github.io/categories/Project/"/>
    
    
    <category term="Object detection" scheme="https://annsonic.github.io/tags/Object-detection/"/>
    
    <category term="RetinaNet" scheme="https://annsonic.github.io/tags/RetinaNet/"/>
    
  </entry>
  
  <entry>
    <title>計數血液細胞-使用HOG和SVM</title>
    <link href="https://annsonic.github.io/2020/10/30/%E8%A8%88%E6%95%B8%E8%A1%80%E6%B6%B2%E7%B4%B0%E8%83%9E-%E4%BD%BF%E7%94%A8HOG%E5%92%8CSVM/"/>
    <id>https://annsonic.github.io/2020/10/30/%E8%A8%88%E6%95%B8%E8%A1%80%E6%B6%B2%E7%B4%B0%E8%83%9E-%E4%BD%BF%E7%94%A8HOG%E5%92%8CSVM/</id>
    <published>2020-10-30T13:56:20.000Z</published>
    <updated>2020-11-05T13:41:07.603Z</updated>
    
    <content type="html"><![CDATA[<p>個人練習，以SVM作為這個題目（數血液細胞個數）的baseline。</p><span id="more"></span><h2 id="主題："><a href="#主題：" class="headerlink" title="主題："></a>主題：</h2><p>計算一張血塗片影像中出現的血小板(Platelet)、紅血球(RBC)和白血球(WBC)的個數。</p><h2 id="假設："><a href="#假設：" class="headerlink" title="假設："></a>假設：</h2><p>玻片的製作、細胞染色和資料標註都符合標準：</p><ol><li>染劑將細胞質和血紅素等染成紅色；細胞核以及白血球的嗜鹼性顆粒染成藍紫色</li><li>位置一半落於邊緣或遮蔽達一半的細胞，不列計算</li></ol><p>如此待處理的問題是：</p><ol><li>不同顯微鏡下的影像都有一些色差</li><li>血球顆粒沒有一定的大小</li><li>血球可能彼此距離很近，看似相連</li></ol><h2 id="實驗限制："><a href="#實驗限制：" class="headerlink" title="實驗限制："></a>實驗限制：</h2><ol><li>資料筆數不多，尤其我又刪掉一些圖片…<br>因為我查到血液中大約有 40%～50%是紅血球，<br>但是有些圖片我看起來有紅血球、但標籤卻完全沒有紅血球，<br>所以我屏除這些紅血球標籤少於5顆的圖片。</li><li>白血球可再細分種類，大略分為五類[1]，<br>此資料集無細分白血球。</li></ol><h2 id="資料集"><a href="#資料集" class="headerlink" title="資料集"></a>資料集</h2><p>檔案來源[2]<br>我自行剔除少數圖片、切割資料集(80%-10%-10%比例)之後，<br>得到的原始統計分佈如下，<br><img src="/2020/10/30/%E8%A8%88%E6%95%B8%E8%A1%80%E6%B6%B2%E7%B4%B0%E8%83%9E-%E4%BD%BF%E7%94%A8HOG%E5%92%8CSVM/statistic_1.png" alt="annotation count and HSV color histogram" title="count statistics, Histogram in HSV space"><br><img src="/2020/10/30/%E8%A8%88%E6%95%B8%E8%A1%80%E6%B6%B2%E7%B4%B0%E8%83%9E-%E4%BD%BF%E7%94%A8HOG%E5%92%8CSVM/stat_w_h_count.png" alt="annotation size" title="size statistics"></p><ol><li>training dataset擁有極多數的紅血球樣本…<br>不過想要平均取樣實在有難度，<br>因為一張影像當中的紅血球的個數變異不小。<br>資料增量方面，我只用水平翻轉、垂直翻轉圖片的方式做。</li><li>有些紅血球的陰影色彩與白血球的顏色相近</li></ol><h2 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h2><p>偵測分為2階段，<br>階段一是從顏色和圓形輪廓來選取可能有血球的畫面，<br>借助watershed[3]演算法來分割擠在一起的血球，<br>階段二是將選取的畫面給SVM分類器過目，<br>SVM[4]分類器根據HOG[5]提取的影像特徵，<br>判斷該區塊是否存在目標細胞。</p><p>SVM分類器以正確截圖的圖片來做訓練。<br><img src="/2020/10/30/%E8%A8%88%E6%95%B8%E8%A1%80%E6%B6%B2%E7%B4%B0%E8%83%9E-%E4%BD%BF%E7%94%A8HOG%E5%92%8CSVM/flow.png" alt="My object detector flow" title="work flow"></p><h2 id="結果"><a href="#結果" class="headerlink" title="結果:"></a>結果:</h2><ol><li><p>分類</p><table><thead><tr><th>Category</th><th>Macro avg F1-score</th></tr></thead><tbody><tr><td>Platelets</td><td>0.96</td></tr><tr><td>RBC</td><td>0.99</td></tr><tr><td>WBC</td><td>0.99</td></tr></tbody></table></li><li><p>偵測<br>使用 VOC-style Average Precision算法[6]。</p><table><thead><tr><th>Category</th><th>AP</th><th>IoU threshold</th></tr></thead><tbody><tr><td>Platelets</td><td>34.32%</td><td>0.1</td></tr><tr><td>RBC</td><td>58.64%</td><td>0.1</td></tr><tr><td>WBC</td><td>91.43%</td><td>0.5</td></tr></tbody></table><p><img src="/2020/10/30/%E8%A8%88%E6%95%B8%E8%A1%80%E6%B6%B2%E7%B4%B0%E8%83%9E-%E4%BD%BF%E7%94%A8HOG%E5%92%8CSVM/mAP.png" alt="average precision" title="the AP score"></p><p>偵測錯誤情況：<br>a. 誤認白血球的細胞質為血小板，這是單純用色板過濾藍紫色區域造成的缺失，<br>   如果能再將周圍的像素也一起評估，分類器才可能看出這是白血球</p>   <img src="fail_Platelet_00003.jpg" width=320 height=240 title="platelet_misclassication" alt="error case" />b. 未能將擠在一起的紅血球分辨清楚，   這是受限於我餵給watershed演算法的圖未能淨化成粒粒分明的圓點...但我也盡力了   <img src="fail_RBC_00013.jpg" width=320 height=240 title="rbc_misclassication" alt="error case" />c. 未帶有核分裂特徵的白血球，不容易被分類器辨識，   猜測是因為其訓練資料少才導致   <img src="fail_WBC_00270.jpg" width=320 height=240 title="wbc_misclassication" alt="error case" /></li></ol><h2 id="除錯心得："><a href="#除錯心得：" class="headerlink" title="除錯心得："></a>除錯心得：</h2><ol><li><p>skimage&#x2F;feature&#x2F;_hog.py, Line 272: ValueError: negative dimensions are not allowed<br>原來我誤會參數的定義了，<br>HOG為了能對光照變化和陰影獲得更好的效果，<br>圖片會切區間(block)，區間內再細分格子細胞(cell)，<br>輸入影像需要配合作resize，或是動態設定pixel_per_cell [7]，<br>為了避免出錯，簡單設定：單一側的image_size &#x3D; 單一側的pixels_per_cell * 單一側的cells_per_block<br><img src="/2020/10/30/%E8%A8%88%E6%95%B8%E8%A1%80%E6%B6%B2%E7%B4%B0%E8%83%9E-%E4%BD%BF%E7%94%A8HOG%E5%92%8CSVM/hog_viz.png" alt="HOG descriptor and parameters" title="visualize the HOG descriptor"></p></li><li><p>這樣訓練的SVM分類器果然很弱，<br>畢竟它只看過截圖截得好好的訓練資料；<br>故意幾張餵細胞的部份截圖，還真的分類錯了。<br>我花費比較多心力在第一階段的畫面選準一點。</p></li><li><p>一開始我是使用sliding window來取ROI給分類器的，<br>超級沒效率，而且對於偵測血小板簡直是海底撈針一樣打不著，<br>才換成用顏色篩選要截圖的區域。</p><p>紅血球有可能擠成一團，<br>參考網路作法用watershed演算法，<br>佩服想出這演算法的大神 👏<br>只是watershed前提需要一個無雜質的背景，<br>轉成灰階檔、濾雜質這步驟就可以來來回回試函數(膨脹、侵蝕…)調整好久…[8]<br>我還嘗試過用Sobel取輪廓 → 距離轉換 → watershed，<br>不過roi格子還是取得很糟、而且超細碎。<br><img src="/2020/10/30/%E8%A8%88%E6%95%B8%E8%A1%80%E6%B6%B2%E7%B4%B0%E8%83%9E-%E4%BD%BF%E7%94%A8HOG%E5%92%8CSVM/roi_flow_364.png" alt="fail_roi_flow" title="sobel_version_roi"></p></li><li><p>我一度納悶，<br>我得到的白血球PR curve是水平線，<br>但為何 AP只有91.43%？<br>原來是因為得到的 recall 最高只有 0.91，<br>模型不能偵測出所有的 ground truth。</p></li></ol><p>應該可以再嘗試負樣本的資料增量，<br>例如在正樣本的周圍取樣、作為負樣本，<br>還有餵給watershed的distance閥值也可以嘗試再調整，<br>大概知道這種做法的痛點了，<br>因為我想把握時間學習deep learning了，<br>所以到此止住。</p><h2 id="參考資料："><a href="#參考資料：" class="headerlink" title="參考資料："></a>參考資料：</h2><p>[1] 血球細胞種類 <a href="http://youthyear.blogspot.com/2010/08/liu-stain.html">link</a><br>[2] 血球細胞資料集 <a href="https://github.com/Shenggan/BCCD_Dataset">link</a><br>[3] Watershed segmentation web <a href="https://www.itread01.com/content/1541250183.html">link</a><br>    Watershed segmentation youtube <a href="https://www.youtube.com/watch?v=AsTvGxuiqKs">link</a><br>[4] SVM演算法的 dual <a href="https://www.quora.com/Whats-the-point-in-using-the-dual-problem-when-fitting-SVM">link 1</a> <a href="https://medium.com/@ashwanibhardwajcodevita16/from-zero-to-hero-in-depth-support-vector-machine-264931a1e135">link 2</a><br>[5] HOG演算法 <a href="https://medium.com/analytics-vidhya/a-take-on-h-o-g-feature-descriptor-e839ebba1e52">link</a><br>[6] VOC-style mAP tool <a href="https://github.com/rafaelpadilla/Object-Detection-Metrics">link</a><br>[7] 依輸入圖檔大小，動態改變HOG函式的pixel_per_cell參數 <a href="https://stackoverflow.com/questions/55664799/default-value-for-pixels-per-cell-skimage-feature-hog/55743702">link</a><br>[8] 框選紅血球的區域 <a href="https://stackoverflow.com/questions/36438313/filling-holes-of-an-image-in-python-with-cv2-not-working">link</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;個人練習，以SVM作為這個題目（數血液細胞個數）的baseline。&lt;/p&gt;</summary>
    
    
    
    <category term="Project" scheme="https://annsonic.github.io/categories/Project/"/>
    
    
    <category term="Object detection" scheme="https://annsonic.github.io/tags/Object-detection/"/>
    
    <category term="histogram oriented gradients (HOG)" scheme="https://annsonic.github.io/tags/histogram-oriented-gradients-HOG/"/>
    
    <category term="SVM" scheme="https://annsonic.github.io/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>造型氣球-美樂蒂</title>
    <link href="https://annsonic.github.io/2020/10/18/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E7%BE%8E%E6%A8%82%E8%92%82/"/>
    <id>https://annsonic.github.io/2020/10/18/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E7%BE%8E%E6%A8%82%E8%92%82/</id>
    <published>2020-10-18T14:05:23.000Z</published>
    <updated>2020-10-18T14:16:35.010Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2020/10/18/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E7%BE%8E%E6%A8%82%E8%92%82/melody.jpg" alt="Melody" title="balloon twisting melody"></p><span id="more"></span><p>依照Balloon Art mania S先生的教學做的~ <a href="https://www.youtube.com/watch?v=pkXU_tVnsQA">教學影片傳送門</a><br>材料:</p><ul><li>身體:白色260氣球，充氣至半條長度</li><li>帽子:粉紅色260氣球，充氣至半條長度</li><li>花蕊:順眼的顏色，260氣球，利用剩料即可</li><li>剪刀</li><li>黑色奇異筆</li><li>黃色奇異筆（我沒有，所以我用黑色畫鼻子的輪廓）</li></ul><p>四泡的可愛造型真是千變萬化啊，<br>佩服想出這些造型的神人們。<br>我也測試了我學習的速度，<br>第一隻美樂蒂邊做邊學、耗了一小時完成，<br>接著第二隻則花不到15分鐘便完成。<br>成就感 成就感 😁</p><p>因為我自己容易倒帶之後反而忘記我目前做到哪一個步驟,<br>所以參考台大氣球社一位學長提出的記譜方式製作這張流程圖<br><img src="/2020/10/18/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E7%BE%8E%E6%A8%82%E8%92%82/g.jpg" alt="flow" title="balloon twisting flow"><br>氣球譜 <a href="https://mropengate.blogspot.com/2016/03/blog-post_87.html">原文</a><br>邊上的文字是表示 – 第幾個步驟:氣泡長度(單位是自己的手指頭寬度）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2020/10/18/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E7%BE%8E%E6%A8%82%E8%92%82/melody.jpg&quot; alt=&quot;Melody&quot; title=&quot;balloon twisting melody&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Balloon Twisting" scheme="https://annsonic.github.io/categories/Balloon-Twisting/"/>
    
    
    <category term="Melody" scheme="https://annsonic.github.io/tags/Melody/"/>
    
  </entry>
  
  <entry>
    <title>造型氣球-四泡版嫦娥</title>
    <link href="https://annsonic.github.io/2020/10/05/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%9B%9B%E6%B3%A1%E7%89%88%E5%AB%A6%E5%A8%A5/"/>
    <id>https://annsonic.github.io/2020/10/05/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%9B%9B%E6%B3%A1%E7%89%88%E5%AB%A6%E5%A8%A5/</id>
    <published>2020-10-05T15:07:37.000Z</published>
    <updated>2020-10-05T15:13:39.094Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2020/10/05/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%9B%9B%E6%B3%A1%E7%89%88%E5%AB%A6%E5%A8%A5/chang_e.jpg" alt="chang_e" title="balloon twisting chang_e"></p><span id="more"></span><p>依照李明波先生的教學做的~ <a href="https://www.youtube.com/watch?v=wkKMiK0QLyM">教學影片傳送門</a><br>材料:</p><ul><li>水袖:白色260氣球，充氣至尾端留4指</li><li>衣服:任意搭配顏色，260氣球，半條長度即可，充氣至尾端留4指</li><li>臉:膚色260氣球，1&#x2F;4條長度即可，充氣至尾端留4指</li><li>髮:黑色260氣球，充氣至尾端留8指</li><li>剪刀</li><li>黑色奇異筆</li><li>紅色奇異筆</li></ul><p>為了消耗之前的黃氣球剩料，<br>所以我選用金髮，<br>變身成外國人，<br>完全是愛麗絲的配色欸（黃、白、水藍），哈哈😁</p><p>成果稍微歪斜，<br>似乎是因為靠近吹嘴的部份比較沒氣…<br>但不知道該如何改善😗</p><p>因為我自己容易倒帶之後反而忘記我目前做到哪一個步驟,<br>所以參考台大氣球社一位學長提出的記譜方式製作這張流程圖<br><img src="/2020/10/05/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%9B%9B%E6%B3%A1%E7%89%88%E5%AB%A6%E5%A8%A5/g.jpg" alt="flow" title="balloon twisting flow"><br>氣球譜 <a href="https://mropengate.blogspot.com/2016/03/blog-post_87.html">原文</a><br>邊上的文字是表示 – 第幾個步驟:氣泡長度(單位是自己的手指頭寬度）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2020/10/05/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%9B%9B%E6%B3%A1%E7%89%88%E5%AB%A6%E5%A8%A5/chang_e.jpg&quot; alt=&quot;chang_e&quot; title=&quot;balloon twisting chang_e&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Balloon Twisting" scheme="https://annsonic.github.io/categories/Balloon-Twisting/"/>
    
    
    <category term="Chang E" scheme="https://annsonic.github.io/tags/Chang-E/"/>
    
  </entry>
  
  <entry>
    <title>Decoupling Representation and Classifier for Long-Tailed Recognition</title>
    <link href="https://annsonic.github.io/2020/09/22/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/"/>
    <id>https://annsonic.github.io/2020/09/22/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/</id>
    <published>2020-09-22T10:34:39.000Z</published>
    <updated>2020-09-22T10:45:02.826Z</updated>
    
    <content type="html"><![CDATA[<p>2020年發表於ICLR，<br>作者來自於Facebook AI，<br>有提供source code<br><a href="https://github.com/facebookresearch/classifier-balancing">https://github.com/facebookresearch/classifier-balancing</a><br>我讀這一篇是因為碰到長尾分佈下的分類問題，<br>試過給loss加權重、但沒幫助，<br>而這篇建議的re-sampling技巧我試過覺得有用，<br>在此紀錄一下自己從中學習到的。</p><span id="more"></span><h2 id="觀點與事實"><a href="#觀點與事實" class="headerlink" title="觀點與事實"></a>觀點與事實</h2><ul><li>此文獻建議以2階段的取樣方式來處理長尾分佈下的分類問題<br><img src="/2020/09/22/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/0.png" alt="method" title="2 stage learning"></li><li>(第一階段)學習好的圖片特徵，<br>以 instance-balanced sampling 方式採樣，<br>一起訓練捲積層和分類器。</li><li>(第二階段)學習分類，<br>固定捲積層不動，<br>以 class-balanced sampling 方式採樣、重新訓練分類器，<br>或是 τ-normalized 將分類器的權重做正規化、τ是超參數，<br>或是 Learnable weight scaling。</li><li>在ImageNet-LT資料集上得到的top-1準確度53.3%(SOTA)，<br>Places-LT上得到top-1準確度37.9%<br>iNaturalist上得到top-1準確度72.5%。</li></ul><h2 id="背景簡介"><a href="#背景簡介" class="headerlink" title="背景簡介"></a>背景簡介</h2><ul><li>長尾分佈如下圖，<br>現實中往往有稀有的類別，<br>難以採集到均衡的資料<br><img src="/2020/09/22/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/1.png" alt="long tailed" title="Imbalanced dataset"></li><li>當數據的分佈極不平衡時，<br>會造成分類器學習到的是”數據的分佈機率”、偏好佔多數的種類。</li><li>資料平衡的可能方法:<ul><li>Oversample the minority class.</li><li>Undersample the majority class.</li><li>Synthesize new minority classes.<br>但是有過擬合、欠擬合的副作用。<br>[1] <a href="https://www.svds.com/learning-imbalanced-classes/">Oversampling and undersampling, Synthesizing new examples: SMOTE</a></li></ul></li><li>可以依類別給予不同的loss權重<br>[1] <a href="https://www.svds.com/learning-imbalanced-classes/">Adjusting class weights</a></li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><ul><li>取樣方式，比較重要的<ul><li>Instance-balanced sampling<br>每個資料點都一樣的機率被選中</li><li>Class-balanced sampling<br>先公平地選擇類別，然後在該類別再公平地選擇資料點<br><img src="/2020/09/22/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/3.png" alt="sampling" title="Sampling strategies"></li></ul></li><li>第二階段的分類器訓練方式<ul><li>Classifier Re-training (cRT)<br>以 class-balanced sampling 方式採樣重新訓練分類器</li><li>τ-normalized<br>將第一階段的分類器的權重做正規化，<br>τ是超參數，作者經由cross validation做選擇</li><li>Learnable weight scaling (LWS)<br>將正規化的係數也透過模型學習，<br>但我不清楚作者用什麼模型來學習<br><img src="/2020/09/22/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/4.png" alt="classifier type" title="Classifier strategies"></li></ul></li></ul><h2 id="實驗結果節錄"><a href="#實驗結果節錄" class="headerlink" title="實驗結果節錄"></a>實驗結果節錄</h2><p>  下圖是作者繪出分類器的權值L2範數對於各類資料的分佈，<br>  橫軸由左至右，是多數類別至少數類別，<br>  黑色虛線表示資料筆數，<br>  藍色線是normal training分類器的權值L2，<br>  可以看到稀有類別的權值比較小，<br>  影響其logit也小，<br>  softmax結果便會偏向佔多數的類別勝出。</p><p>  而綠色線是cRT得到分類器的權值L2，<br>  它犧牲多數類別的權值，<br>  稀有類別的權值比較高。</p><p>  τ-norm(黃線) 和 LWS(棕線)表現差不多，<br>  他們對權重的平衡比較折衷。</p><p>  <img src="/2020/09/22/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/2.png" alt="long tailed issue" title="Imbalanced dataset issue"></p><ul><li>與前作的分類準確度比較<br><img src="/2020/09/22/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/5.png" alt="imagenet-lt" title="imagenet-lt exp."><br><img src="/2020/09/22/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/6.png" alt="iNaturalist" title="iNaturalist exp."></li></ul><h2 id="個人感想"><a href="#個人感想" class="headerlink" title="個人感想"></a>個人感想</h2><ul><li>容易搭配，<br>因為只須改變sampling方式和調整fully connect layer，<br>可以與不同的backbone架構搭配。</li><li>Class-balanced sampling是最簡便的，<br>只是應該會影響batch size的選擇…<br>例如batch size會是類別數目的倍數，<br>又，等效訓練batch數變多(拉長訓練時間)</li><li>我用在我的文字情緒分類問題上，有效，<br>獻醜了，<br>我的資料筆數：230, 210, 150, 60, 60, 50 … 12，<br>是長尾分佈，<br>feature extrator是拿別人pre-trained好，<br>我只訓練fully connect layer。<br>這是Instance-balanced sampling得到的混搖矩陣<br><img src="/2020/09/22/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/7.png" alt="ib" title="ib exp."><br>這是Class-balanced sampling得到的混搖矩陣<br><img src="/2020/09/22/Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition/8.png" alt="cb" title="ib exp."><br>兩者準確率沒差，<br>但Class-balanced sampling的precision較均衡、f1-score高了10%。</li></ul><h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><p>[1] 資料不平衡時的訓練技巧 <a href="https://www.svds.com/learning-imbalanced-classes/">https://www.svds.com/learning-imbalanced-classes/</a><br>[2] 論文心得 <a href="https://zhuanlan.zhihu.com/p/158638078">https://zhuanlan.zhihu.com/p/158638078</a><br>[3] 論文心得 <a href="https://blog.csdn.net/shanglianlm/article/details/105973699">https://blog.csdn.net/shanglianlm/article/details/105973699</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2020年發表於ICLR，&lt;br&gt;作者來自於Facebook AI，&lt;br&gt;有提供source code&lt;br&gt;&lt;a href=&quot;https://github.com/facebookresearch/classifier-balancing&quot;&gt;https://github.com/facebookresearch/classifier-balancing&lt;/a&gt;&lt;br&gt;我讀這一篇是因為碰到長尾分佈下的分類問題，&lt;br&gt;試過給loss加權重、但沒幫助，&lt;br&gt;而這篇建議的re-sampling技巧我試過覺得有用，&lt;br&gt;在此紀錄一下自己從中學習到的。&lt;/p&gt;</summary>
    
    
    
    <category term="Interested Paper" scheme="https://annsonic.github.io/categories/Interested-Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>造型氣球-棒棒糖</title>
    <link href="https://annsonic.github.io/2020/09/20/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E6%A3%92%E6%A3%92%E7%B3%96/"/>
    <id>https://annsonic.github.io/2020/09/20/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E6%A3%92%E6%A3%92%E7%B3%96/</id>
    <published>2020-09-20T14:26:29.000Z</published>
    <updated>2020-09-20T14:56:04.888Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2020/09/20/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E6%A3%92%E6%A3%92%E7%B3%96/lollipop.jpg" alt="lollipop" title="balloon twisting lollipop"></p><span id="more"></span><p>參考Smile for Kids先生的教學做的~ <a href="https://www.youtube.com/watch?v=coZaEQrSNis">教學影片傳送門</a><br>材料:</p><p>氣球顏色可以任意搭配</p><ul><li>外圈:白色260氣球，充氣至尾端留5指</li><li>內圈:碧色260氣球，充氣至尾端留5指</li><li>剪刀</li></ul><p>Smile for Kids先生的步驟比較特別的是從雙泡做出雙熊耳結（影片1:00處），<br>然後才是綁上第二條氣球，<br>另外一層一層裹上的氣球…是量好長度後、做成環結、才將內層氣球”捲”進環結裡，<br>我感覺這樣做出來的棒棒糖”盤面”比較緊實、平整，<br>可是我挑戰雙熊耳結失敗，<br>不知道是不是因為我的氣球球皮彈性不夠，<br>壓不扁雙泡，<br>只好依老習慣做熊耳結。</p><p>最後錦上添花，打上蝴蝶結，當禮物，嘿~</p><p>因為我自己容易倒帶之後反而忘記我目前做到哪一個步驟,<br>所以參考台大氣球社一位學長提出的記譜方式製作這張流程圖<br><img src="/2020/09/20/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E6%A3%92%E6%A3%92%E7%B3%96/g.jpg" alt="flow" title="balloon twisting flow"><br>氣球譜 <a href="https://mropengate.blogspot.com/2016/03/blog-post_87.html">原文</a><br>邊上的文字是表示 – 第幾個步驟:氣泡長度(單位是自己的手指頭寬度）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2020/09/20/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E6%A3%92%E6%A3%92%E7%B3%96/lollipop.jpg&quot; alt=&quot;lollipop&quot; title=&quot;balloon twisting lollipop&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Balloon Twisting" scheme="https://annsonic.github.io/categories/Balloon-Twisting/"/>
    
    
    <category term="lollipop" scheme="https://annsonic.github.io/tags/lollipop/"/>
    
  </entry>
  
  <entry>
    <title>第一課-成功老化</title>
    <link href="https://annsonic.github.io/2020/09/11/%E7%AC%AC%E4%B8%80%E8%AA%B2-%E6%B4%BB%E8%BA%8D%E8%80%81%E5%8C%96/"/>
    <id>https://annsonic.github.io/2020/09/11/%E7%AC%AC%E4%B8%80%E8%AA%B2-%E6%B4%BB%E8%BA%8D%E8%80%81%E5%8C%96/</id>
    <published>2020-09-11T11:03:04.000Z</published>
    <updated>2020-09-11T12:06:40.573Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2020/09/11/%E7%AC%AC%E4%B8%80%E8%AA%B2-%E6%B4%BB%E8%BA%8D%E8%80%81%E5%8C%96/g.jpg" alt="successful aging" title="factors of successful aging"></p><span id="more"></span><p>長照學程當然得了解老年生活的品質該如何定義，<br>有若干的定義，<br>成功老化是最早(Rowe 與Kahn(1987))被提出、被廣泛引用的，<br>如圖包含三個因子，<br>包含生理、心理和社會三個層面，<br>三者交集俱皆達成時即為最成功的老化狀況。<br>在生理方面維持良好的健康及獨立自主的生活；<br>在心理方面適應良好無憂鬱症狀，認知功能正常；<br>在社會方面維持良好的家庭及社會關係，<br>讓身心靈保持最佳的狀態，<br>進而享受老年的生活。<br>Crowther 等人(2002)將三因子模型延伸，<br>提出正面靈性，<br>定義為宗教（religion）與靈性（spirituality）之特性所導致的正面結果。</p><p>WHO於2002 年提出活躍老化的觀念，<br>並定義為「提升民眾老年期生活品質，並達到最適宜的健康、社會參與及安全的過程。」</p><p>我比較喜歡活躍老化這個稱呼，<br>因為以前的觀念是老人家應該待在家養老，<br>活躍老化則從字面上提醒，老年人仍可以在社會家庭發揮能力喔。<br>但成功老化、活躍老化都是概念，<br>並沒有量化評量標準。</p><hr><p>第一次報告的作業也很有趣，<br>學生自行找一位老人家、訪問老人通常一天做哪些活動，<br>與年輕人的一日生活相比較，<br>以及討論該位老人家是否仍有參與社會事務，<br>老人感興趣的休閒活動是什麼？</p><p>我們這組訪問組員的阿嬤，<br>85歲，獨居，能獨立出門、自理生活，<br>她起床後抄經、打掃、公園運動，<br>午後去銀髮俱樂部、有時候社區共餐，<br>傍晚又去公園運動、和其他阿公阿嬤聯絡感情，<br>有些項目已經持續多年了，<br>阿嬤也是很早便達到經濟獨立、退休。</p><p>其他組採訪到的長者尚未退休，<br>生活的重心仍然在工作，<br>經濟狀況好的才會留時間做運動，<br>這也讓我認識到雖然都算是老人…<br>65歲的長者覺得自己不算老哩，<br>而健康狀況也造成老年人彼此之間的差異性大。</p><p>另外可能因為阿嬤是住在都市的老人，<br>我們可以用國語跟她順暢溝通，<br>阿嬤接觸到的社區資源也不少（松年大學、社區共餐），<br>跟我印象中、說著台語、只在家裡看電視的老人很大的不同，<br>讓我破除偏見。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2020/09/11/%E7%AC%AC%E4%B8%80%E8%AA%B2-%E6%B4%BB%E8%BA%8D%E8%80%81%E5%8C%96/g.jpg&quot; alt=&quot;successful aging&quot; title=&quot;factors of successful aging&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Geriatric Welfare" scheme="https://annsonic.github.io/categories/Geriatric-Welfare/"/>
    
    
  </entry>
  
  <entry>
    <title>造型氣球-綿羊</title>
    <link href="https://annsonic.github.io/2020/09/06/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E7%B6%BF%E7%BE%8A/"/>
    <id>https://annsonic.github.io/2020/09/06/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E7%B6%BF%E7%BE%8A/</id>
    <published>2020-09-06T09:27:11.000Z</published>
    <updated>2020-09-06T09:39:52.194Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2020/09/06/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E7%B6%BF%E7%BE%8A/sheep.jpg" alt="sheep" title="balloon twisting sheep"></p><span id="more"></span><p>參考Changsunny小姐的教學做的~ <a href="https://www.youtube.com/watch?v=yw9ljrlodQE">教學影片傳送門</a><br>材料:</p><ul><li>臉:膚色6吋心型氣球，充氣3下</li><li>頭髮:白色260氣球，充氣留8指</li><li>身體:白色5吋圓形氣球，充氣3下</li><li>腿:白色260氣球半條*2，充氣留4指</li><li>蹄:棕色260氣球1&#x2F;4條*4，充氣2下</li><li>剪刀</li><li>黑色奇異筆</li></ul><p>這裡我的計算：打氣筒推出去算是1下、拉回來也算是1下哦。</p><p>一直很想做這個造型，它讓我想起我小時候的綿羊玩偶！<br>不過需要很多顆氣球，製作成本有點高，哈。<br>我的羊太瘦、空隙很多，<br>因為我看錯打氣的次數…都少打1下，<br>趕緊記下來，<br>下次做修正！</p><p>因為我自己容易倒帶之後反而忘記我目前做到哪一個步驟,<br>所以參考台大氣球社一位學長提出的記譜方式製作這張流程圖<br><img src="/2020/09/06/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E7%B6%BF%E7%BE%8A/g.jpg" alt="flow" title="balloon twisting flow"><br>氣球譜 <a href="https://mropengate.blogspot.com/2016/03/blog-post_87.html">原文</a><br>邊上的文字是表示 – 第幾個步驟:氣泡長度<br>(長條單位是自己的手指頭寬度，圓形氣球單位是打氣次數）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2020/09/06/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E7%B6%BF%E7%BE%8A/sheep.jpg&quot; alt=&quot;sheep&quot; title=&quot;balloon twisting sheep&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Balloon Twisting" scheme="https://annsonic.github.io/categories/Balloon-Twisting/"/>
    
    
    <category term="sheep" scheme="https://annsonic.github.io/tags/sheep/"/>
    
  </entry>
  
  <entry>
    <title>長照學程粗淺資訊整理</title>
    <link href="https://annsonic.github.io/2020/09/02/%E9%95%B7%E7%85%A7%E5%AD%B8%E7%A8%8B%E7%B2%97%E6%B7%BA%E8%B3%87%E8%A8%8A%E6%95%B4%E7%90%86/"/>
    <id>https://annsonic.github.io/2020/09/02/%E9%95%B7%E7%85%A7%E5%AD%B8%E7%A8%8B%E7%B2%97%E6%B7%BA%E8%B3%87%E8%A8%8A%E6%95%B4%E7%90%86/</id>
    <published>2020-09-02T08:19:10.000Z</published>
    <updated>2020-09-02T09:35:54.570Z</updated>
    
    <content type="html"><![CDATA[<p>第一篇筆記先整理科系與工作議題，以及一些網路媒體資源。</p><span id="more"></span><h2 id="網路媒體資源"><a href="#網路媒體資源" class="headerlink" title="網路媒體資源"></a>網路媒體資源</h2><p>台灣的福祉科技與服務管理學刊(可免費下載閱讀) <a href="http://journal.gerontechnology.org.tw/">傳送門</a></p><h2 id="科系列表"><a href="#科系列表" class="headerlink" title="科系列表"></a>科系列表</h2><p>老人照顧相關科系之相關大專校院名單 <a href="https://ulist.moe.gov.tw/Query/AjaxQuery/Discipline/0921">傳送門</a></p><h2 id="工作議題"><a href="#工作議題" class="headerlink" title="工作議題"></a>工作議題</h2><ul><li><p>照護</p><ul><li>老年醫學、家醫科：老年症候群(譫妄、跌倒、尿失禁、失智、營養不良、不良於行、骨質疏鬆等)</li><li>精神科、神經內科、認知心理學：失智症、憂鬱症等</li><li>公衛：健康促進、社區預防</li><li>物理治療：身體的復健、居家照護</li><li>職能治療：日常生活的復能、居家照護、活動設計</li><li>語言治療與聽力:語言治療</li><li>護理：營養、居家照護</li><li>社工：福利政策、健康保險管理、活動設計</li><li>景觀園藝：園藝治療</li><li>機械、材料：設計輔具</li><li>室內設計：高齡友善空間</li></ul></li><li><p>健康管理</p><ul><li>健康管理、物理治療、運動學系：體適能計畫、休閒規劃</li><li>財務金融：理財規劃</li><li>人力資源：職務再設計、志工</li><li>電機資工：心肺資料監測、跌倒偵測、建立遠距醫療環境</li></ul></li></ul><p>過去一年我修了3門屬於長照學程的課，<br>大家都驚訝怎麼有電機系學生來修這種課，<br>我覺得雖然不是授予我求職一技之長的課程，<br>但是修課讓我轉化對老年”負面”觀念，<br>課程中接觸了”活躍老化”的概念，<br>以及小組作業中與不同年齡、背景的長輩互動，<br>這些都讓我覺得我得到一個學習榜樣，<br>別害怕變老，<br>要朝著過理想的人生邁進。</p><p>電機資工在這塊長照領域的角色是什麼？<br>看來看去比較實用的是做物聯網(心肺資料監測、跌倒偵測)，<br>我要繼續收集比較創新實用的工作項目。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一篇筆記先整理科系與工作議題，以及一些網路媒體資源。&lt;/p&gt;</summary>
    
    
    
    <category term="Geriatric Welfare" scheme="https://annsonic.github.io/categories/Geriatric-Welfare/"/>
    
    
  </entry>
  
  <entry>
    <title>Dynamic Knowledge Graph Construction for Zero-shot Commonsense Question Answering</title>
    <link href="https://annsonic.github.io/2020/09/01/Dynamic-Knowledge-Graph-Construction-for-Zero-shot-Commonsense-Question-Answering/"/>
    <id>https://annsonic.github.io/2020/09/01/Dynamic-Knowledge-Graph-Construction-for-Zero-shot-Commonsense-Question-Answering/</id>
    <published>2020-09-01T11:57:24.000Z</published>
    <updated>2020-09-20T14:53:06.877Z</updated>
    
    <content type="html"><![CDATA[<p>2019年發表於ArXiv，<br>作者來自於艾倫人工智慧研究所，<br>無提供source code<br>不過其中運用了他們的前作COMeT[1]，<br>COMeT有提供source code: <a href="https://github.com/atcbosselut/comet-commonsense">https://github.com/atcbosselut/comet-commonsense</a><br>和網頁互動demo: <a href="https://mosaickg.apps.allenai.org/">https://mosaickg.apps.allenai.org/</a><br>我讀這一篇是因為做情感分類的題目，<br>想知道如何利用COMeT提供的資訊來輔助分類文字的情緒，<br>紀錄一下自己從中學習到的。</p><span id="more"></span><h2 id="觀點與事實"><a href="#觀點與事實" class="headerlink" title="觀點與事實"></a>觀點與事實</h2><ul><li>此文獻提出使用動態生成的知識圖譜來回答社交上的常識性問題，達成零樣本學習的模型</li><li>可以應用於社交機器人的決策，或是應答</li><li>作者利用COMeT模型，<br>並加以計算知識圖譜的multi-hop推論。</li><li>在STORY Commonsense dataset上測試情緒分類，<br>1-hop推論得到的F1-score &#x3D; 19.3%，<br>相較於直接使用COMeT的結果好1%。</li></ul><h2 id="背景簡介"><a href="#背景簡介" class="headerlink" title="背景簡介"></a>背景簡介</h2><ul><li>可以將分類問題，轉化為QA問題，而且使用知識圖譜，計算multi-hop推論，選出信心值最高的答案<br><img src="/2020/09/01/Dynamic-Knowledge-Graph-Construction-for-Zero-shot-Commonsense-Question-Answering/1.png" alt="scenario" title="Application"></li><li>前作ATOMIC[2]提出一個事件和隱含的社交常識對應的資料集，<br>標註了事件主角的感受(xReact)<br><img src="/2020/09/01/Dynamic-Knowledge-Graph-Construction-for-Zero-shot-Commonsense-Question-Answering/2.png" alt="atomic" title="ATOMIC dataset"><br><img src="/2020/09/01/Dynamic-Knowledge-Graph-Construction-for-Zero-shot-Commonsense-Question-Answering/3.png" alt="atomic items" title="ATOMIC dataset items"><br>有提供網頁以瀏覽資料集： <a href="https://homes.cs.washington.edu/~msap/atomic/">https://homes.cs.washington.edu/~msap/atomic/</a></li><li>前作COMeT[1]使用GPT模型、訓練在ATOMIC資料集[2]，<br>將ATOMIC的事件、標註類型和標註值串接成一個句子，<br>所以給個起頭的句子(即事件和標註類型)，<br>GPT模型會接著推論標註值來完成句子，<br>所以是動態的知識圖譜。<br><img src="/2020/09/01/Dynamic-Knowledge-Graph-Construction-for-Zero-shot-Commonsense-Question-Answering/4.png" alt="comet" title="COMeT"></li></ul><h2 id="模型與方法"><a href="#模型與方法" class="headerlink" title="模型與方法"></a>模型與方法</h2><ul><li>升級COMeT，使用GPT-2作為骨幹，<br>藉由GPT-2已讀過許多書，<br>能廣泛推論不在ATOMIC資料集內的事件，<br>有潛力達成Zero-shot learning</li><li>multi-hop推論來增進推論的準確率，<br>作者的認為可以由xWant、xIntent、xEffect等標註類型，<br>來推論背後的主角的感受(xReact)，<br>但因為知識圖譜上路徑太多，<br>需要手動設定關聯性閥值以取捨路徑<br><img src="/2020/09/01/Dynamic-Knowledge-Graph-Construction-for-Zero-shot-Commonsense-Question-Answering/5.png" alt="direct inference" title="CA"><br><img src="/2020/09/01/Dynamic-Knowledge-Graph-Construction-for-Zero-shot-Commonsense-Question-Answering/6.png" alt="1-hop" title="CGA"></li></ul><h2 id="資料集"><a href="#資料集" class="headerlink" title="資料集"></a>資料集</h2><ul><li>我只挑情緒分類這一個實驗來介紹<br>作者使用STORY COMMONSENSE Dataset做驗證，<br>它以5句構成一則小故事，<br>句子有標註幾個描述情感的形容詞，<br>training dataset是標註多個情感強烈程度，<br>validation和testing dataset才是標註單一個情緒分類<br><img src="/2020/09/01/Dynamic-Knowledge-Graph-Construction-for-Zero-shot-Commonsense-Question-Answering/7.png" alt="story_commonsense" title="story"></li></ul><h2 id="實驗結果節錄"><a href="#實驗結果節錄" class="headerlink" title="實驗結果節錄"></a>實驗結果節錄</h2><p>  <img src="/2020/09/01/Dynamic-Knowledge-Graph-Construction-for-Zero-shot-Commonsense-Question-Answering/9.png" alt="story_commonsense_exp" title="story exp."></p><h2 id="個人感想"><a href="#個人感想" class="headerlink" title="個人感想"></a>個人感想</h2><ul><li><p>COMeT能接受的句型有限，應該是受限於ATOMIC資料集句子、偏向簡短的SVO句型</p></li><li><p>實驗中如何得到情緒分類的標籤…關於這點作者沒有交待清楚，<br>COMeT產生的xReact不是完全照情緒分類的標籤名稱，<br>像是這樣<br><img src="/2020/09/01/Dynamic-Knowledge-Graph-Construction-for-Zero-shot-Commonsense-Question-Answering/8.png" alt="comet output" title="my exp."><br>我查了一下別的QA論文，<br>Language Models are Unsupervised Multitask Learners這一篇當中的Natural Questions dataset驗證，<br>它的QA答案是藏在context文字中，<br>模型的輸出是答案在context文字中的index值；<br>可是STORY COMMONSENSE Dataset的答案並不是藏在context文字裡欸，<br>想像上會需要有個字典，<br>將COMeT輸出的形容詞再分類</p></li><li><p>作者比較了supervised learning和zero-shot learning，<br>GPT(supervised learning)的結果大勝COMeT(zero-shot learning)，<br>我感覺因為COMeT輸出的形容詞會有分歧的狀況，<br>例如退休事件、輸出了疲憊和喜悅，<br>再推論單一個情緒分類標籤時會產生干擾，<br>supervised learning則專心學習出一個情緒分類標籤。</p><p>我不能冤枉COMeT不好，<br>它應該適用於申論題的情況，<br>能考慮到多個面向的答案。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;2019年發表於ArXiv，&lt;br&gt;作者來自於艾倫人工智慧研究所，&lt;br&gt;無提供source code&lt;br&gt;不過其中運用了他們的前作COMeT[1]，&lt;br&gt;COMeT有提供source code: &lt;a href=&quot;https://github.com/atcbosselut/comet-commonsense&quot;&gt;https://github.com/atcbosselut/comet-commonsense&lt;/a&gt;&lt;br&gt;和網頁互動demo: &lt;a href=&quot;https://mosaickg.apps.allenai.org/&quot;&gt;https://mosaickg.apps.allenai.org/&lt;/a&gt;&lt;br&gt;我讀這一篇是因為做情感分類的題目，&lt;br&gt;想知道如何利用COMeT提供的資訊來輔助分類文字的情緒，&lt;br&gt;紀錄一下自己從中學習到的。&lt;/p&gt;</summary>
    
    
    
    <category term="Interested Paper" scheme="https://annsonic.github.io/categories/Interested-Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>造型氣球-佩佩豬-2</title>
    <link href="https://annsonic.github.io/2020/08/31/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E4%BD%A9%E4%BD%A9%E8%B1%AC-2/"/>
    <id>https://annsonic.github.io/2020/08/31/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E4%BD%A9%E4%BD%A9%E8%B1%AC-2/</id>
    <published>2020-08-31T05:06:38.000Z</published>
    <updated>2020-08-31T05:16:07.237Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2020/08/31/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E4%BD%A9%E4%BD%A9%E8%B1%AC-2/peppa_pig_2.jpg" alt="peppa_pig" title="balloon twisting peppa_pig_2"></p><span id="more"></span><p>參考3位老師的教學做的~<br>沒辦法…這造型對我來說太複雜了，還是多多綜合、取教學中我吸收的了的部份囉。<br>阿姨的<a href="https://www.youtube.com/watch?v=x7TohaHndSM">教學影片傳送門</a><br>氣球君的頭部<a href="https://www.youtube.com/watch?v=aRov7B6HJrM">教學影片傳送門</a><br>Sergey Loginov的身體<a href="https://www.youtube.com/watch?v=jzSIfwQ_Adw">教學影片傳送門</a><br>Sergey Loginov的底座<a href="https://www.youtube.com/watch?v=-Ls2DCk3Uxw">教學影片傳送門</a></p><p>材料:</p><ul><li>頭：粉紅12吋心型氣球，充氣13下</li><li>衣服:紅色260氣球*2，我是用藍色，充氣至尾端留5指</li><li>手:粉紅色160氣球*2，充氣至尾端留8指</li><li>腿:粉紅色260氣球，充氣至尾端留8指</li><li>鞋:黑色260氣球，我是用黃色，充氣至尾端留8指</li><li>剪刀</li><li>黑色奇異筆</li><li>紅色奇異筆</li><li>白色貼紙</li><li>黑色貼紙，我則是剪白紙、塗上黑色眼珠，再用雙面膠貼上</li></ul><p>這隻我花了2小時做出來，<br>衣服的作法讓我困惑、反覆倒帶研究影片很久，<br>最後將流程圖畫出來之後，<br>感覺又沒那麼複雜。<br>不過最開心的是，<br>我朋友認得出來這隻是佩佩豬~<br>我想說我沒有很忠於原作的配色(還忘記做尾巴…)，呀比～</p><p>因為我自己容易倒帶之後反而忘記我目前做到哪一個步驟,<br>所以參考台大氣球社一位學長提出的記譜方式製作這張流程圖<br><img src="/2020/08/31/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E4%BD%A9%E4%BD%A9%E8%B1%AC-2/g.jpg" alt="flow" title="balloon twisting flow"><br>氣球譜 <a href="https://mropengate.blogspot.com/2016/03/blog-post_87.html">原文</a><br>邊上的文字是表示 – 第幾個步驟:氣泡長度(單位是自己的手指頭寬度）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2020/08/31/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E4%BD%A9%E4%BD%A9%E8%B1%AC-2/peppa_pig_2.jpg&quot; alt=&quot;peppa_pig&quot; title=&quot;balloon twisting peppa_pig_2&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Balloon Twisting" scheme="https://annsonic.github.io/categories/Balloon-Twisting/"/>
    
    
    <category term="Peppa Pig" scheme="https://annsonic.github.io/tags/Peppa-Pig/"/>
    
  </entry>
  
  <entry>
    <title>造型氣球-小公雞</title>
    <link href="https://annsonic.github.io/2020/06/27/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%B0%8F%E5%85%AC%E9%9B%9E/"/>
    <id>https://annsonic.github.io/2020/06/27/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%B0%8F%E5%85%AC%E9%9B%9E/</id>
    <published>2020-06-27T05:08:06.000Z</published>
    <updated>2020-08-31T08:03:39.583Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2020/06/27/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%B0%8F%E5%85%AC%E9%9B%9E/rooster.jpg" alt="rooster" title="balloon twisting rooster"></p><span id="more"></span><p>參考李明波先生的教學做的~ <a href="https://www.youtube.com/watch?v=gllDu2FH_GQ">教學影片傳送門</a><br>材料:</p><ul><li>身體:白色260氣球，充氣至尾端留8指</li><li>喙足:黃色260氣球，充氣至半條長</li><li>雞冠:紅色260氣球，充氣至半條長</li><li>剪刀</li><li>黑色奇異筆</li></ul><p>非常推薦這個造型!!!<br>簡單做、又可愛，<br>帶給我滿滿成就感。<br>因為比較喜歡圓滾滾的感覺，我便略過雞爪了。</p><p>因為我自己容易倒帶之後反而忘記我目前做到哪一個步驟,<br>所以參考台大氣球社一位學長提出的記譜方式製作這張流程圖<br><img src="/2020/06/27/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%B0%8F%E5%85%AC%E9%9B%9E/g.jpg" alt="flow" title="balloon twisting flow"><br>氣球譜 <a href="https://mropengate.blogspot.com/2016/03/blog-post_87.html">原文</a><br>邊上的文字是表示 – 第幾個步驟:氣泡長度(單位是自己的手指頭寬度）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2020/06/27/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%B0%8F%E5%85%AC%E9%9B%9E/rooster.jpg&quot; alt=&quot;rooster&quot; title=&quot;balloon twisting rooster&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Balloon Twisting" scheme="https://annsonic.github.io/categories/Balloon-Twisting/"/>
    
    
    <category term="rooster" scheme="https://annsonic.github.io/tags/rooster/"/>
    
  </entry>
  
  <entry>
    <title>Jointly Optimizing Diversity and Relevance in Neural Response Generation</title>
    <link href="https://annsonic.github.io/2020/05/09/Jointly-Optimizing-Diversity-and-Relevance-in-Neural-Response-Generation/"/>
    <id>https://annsonic.github.io/2020/05/09/Jointly-Optimizing-Diversity-and-Relevance-in-Neural-Response-Generation/</id>
    <published>2020-05-09T03:48:22.000Z</published>
    <updated>2020-08-31T10:21:22.776Z</updated>
    
    <content type="html"><![CDATA[<p>2019年發表於NLP四大頂會之一的NAACL-HLT，<br>作者來自於微軟，<br>有提供source code:<a href="https://github.com/golsun/SpaceFusion">https://github.com/golsun/SpaceFusion</a></p><span id="more"></span><h2 id="觀點與事實"><a href="#觀點與事實" class="headerlink" title="觀點與事實"></a>觀點與事實</h2><ul><li>此文獻提出一個控制Seq2seq模型生成語意的方法</li><li>可以應用於做出有個性的聊天機器人、給對話加料</li><li>作者提出SpaceFusion模型，<br>在loss函數引入新的regularization項，<br>將AE模型的encoder潛在空間融入Seq2Seq模型的空間,<br>可以藉由在encoder潛在空間以內插的方式，<br>控制生成的句子的語氣或意見。</li><li>請評審評分（5分制），<br>SpaceFusion模型得到句子相關性2.72分，<br>變化性2.53分，<br>較其他模型優。</li></ul><h2 id="背景簡介"><a href="#背景簡介" class="headerlink" title="背景簡介"></a>背景簡介</h2><ul><li>已有Seq2seq模型用於聊天機器人（自然語言生成）的任務，<br>但存在問題之一是容易產生無意義的句子，<br>例如：I don’t know，或是OK，<br>原因是訓練文本中很多這種無意義句子，<br>而這些句子泛用於多種情境中。</li><li>句子的多樣性與對話的相關性，往往是需要折衷的。</li><li>前作[2]使用AE輔助Seq2Seq，<br>兩者共用同一個decoder，<br><img src="/2020/05/09/Jointly-Optimizing-Diversity-and-Relevance-in-Neural-Response-Generation/1.jpg" alt="previous work" title="AE multitask"><br>Seq2Seq輸入context句子，<br>學習輸出對應的hypothesis句子，<br>AE負責學習其他可以配上此context的reference句子、學習出這些句子的空間分佈，<br>兩者交替學習；<br><img src="/2020/05/09/Jointly-Optimizing-Diversity-and-Relevance-in-Neural-Response-Generation/2.jpg" alt="latent space" title="latent space"><br>但是將Seq2seq和AE的encoder潛在空間一起視覺化後，<br>發現兩者沒有交集，<br>空間中存在一個大空白區域。</li></ul><h2 id="資料集"><a href="#資料集" class="headerlink" title="資料集"></a>資料集</h2><ul><li>作者使用Switchboard資料集和Reddit資料集，我針對Reddit資料集做介紹。<br>從Reddit網站爬了2011年份的發文以及其留言，<br>以發文作為context、留言作為references句子（和hypothesis句子），<br>平均一句context有24句reference。<br><img src="/2020/05/09/Jointly-Optimizing-Diversity-and-Relevance-in-Neural-Response-Generation/3.jpg" alt="dataset" title="reddit"><h2 id="模型與方法"><a href="#模型與方法" class="headerlink" title="模型與方法"></a>模型與方法</h2><ol><li>為了消弭空間中的空白區域，<br>  將Seq2Seq的encoder潛在空間與AE的encoder潛在空間拉近，<br>  因為句子配對有n種組合、所以距離除以n，n是batch size;<br>  <img src="/2020/05/09/Jointly-Optimizing-Diversity-and-Relevance-in-Neural-Response-Generation/4.jpg" alt="method 1" title="regularization "></li><li>為求句子均勻佔據在空間中，<br>  不同的context彼此在Seq2seq encoder潛在空間中的距離要拉遠，<br>  同樣地，references句子彼此在AE encoder潛在空間中的距離要拉遠，<br>  而句子佔據空間中有 n*(n-1)種組合，<br>  所以距離除以n^2-n<br>  <img src="/2020/05/09/Jointly-Optimizing-Diversity-and-Relevance-in-Neural-Response-Generation/5.jpg" alt="method 2" title="regularization "></li><li>希望能以內插的方式，控制生成的句子的語氣或意見，<br>  所以加入新的內插encoder潛在空間Zinterp，<br>  Zinterp的計算公式如下，<br>  decoder實際為3個不同的decoder所組成。<br>  發現一個疑點，<br>  Zinterp生成的句子yinterp_hat是需要標準答案，<br>  才能計算出categorical cross entropy，<br>  這…看code還是不清楚作者怎麼解決此問題的，<br>  不過看作者實驗中的u值，<br>  u超過0.3即取reference句子做輸出，<br>  可能就是這般訓練出模型的。</li></ol><p>4.整體的loss函數，於是Seq2seq和AE變成同時做訓練。<br>  <img src="/2020/05/09/Jointly-Optimizing-Diversity-and-Relevance-in-Neural-Response-Generation/6.jpg" alt="method 3" title="regularization "><br>  <img src="/2020/05/09/Jointly-Optimizing-Diversity-and-Relevance-in-Neural-Response-Generation/7.jpg" alt="model" title="model "></p><h2 id="實驗結果節錄"><a href="#實驗結果節錄" class="headerlink" title="實驗結果節錄"></a>實驗結果節錄</h2><p><img src="/2020/05/09/Jointly-Optimizing-Diversity-and-Relevance-in-Neural-Response-Generation/8.jpg" alt="result" title="result "><br><img src="/2020/05/09/Jointly-Optimizing-Diversity-and-Relevance-in-Neural-Response-Generation/9.jpg" alt="result" title="result "><br><img src="/2020/05/09/Jointly-Optimizing-Diversity-and-Relevance-in-Neural-Response-Generation/10.jpg" alt="result" title="result "><br><img src="/2020/05/09/Jointly-Optimizing-Diversity-and-Relevance-in-Neural-Response-Generation/11.jpg" alt="result" title="result "><br><img src="/2020/05/09/Jointly-Optimizing-Diversity-and-Relevance-in-Neural-Response-Generation/12.jpg" alt="result" title="result "></p></li></ul><h2 id="個人感想"><a href="#個人感想" class="headerlink" title="個人感想"></a>個人感想</h2><ul><li>作者擅長視覺化說故事</li><li>將encoder潛在空間混合後，用內插法來控制生成句子的語氣或意見，<br>感覺比起其他方法來得溫和，句子品質應該有比較好</li><li>但內插u的值域不是預期的[0, 1]、而是[0, 0.3]，<br>造成內插帶來的漸變效果有限。</li></ul><h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><p>[1] “Jointly Optimizing Diversity and Relevance in Neural Response Generation,” Xiang Gao, Sungjin Lee, Yizhe Zhang, Chris Brockett, Michel Galley, Jianfeng Gao, Bill Dolan, NAACL-HLT, 2019.<br>[2] “Multi-Task Learning for Speaker-Role Adaptation in Neural Conversation Models,” Yi Luan, Chris Brockett, Bill Dolan, Jianfeng Gao, Michel Galley, IJCNLP 2017.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2019年發表於NLP四大頂會之一的NAACL-HLT，&lt;br&gt;作者來自於微軟，&lt;br&gt;有提供source code:&lt;a href=&quot;https://github.com/golsun/SpaceFusion&quot;&gt;https://github.com/golsun/SpaceFusion&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Interested Paper" scheme="https://annsonic.github.io/categories/Interested-Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>造型氣球-佩佩豬</title>
    <link href="https://annsonic.github.io/2020/05/04/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E4%BD%A9%E4%BD%A9%E8%B1%AC/"/>
    <id>https://annsonic.github.io/2020/05/04/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E4%BD%A9%E4%BD%A9%E8%B1%AC/</id>
    <published>2020-05-03T16:17:52.000Z</published>
    <updated>2020-08-31T05:04:38.516Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2020/05/04/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E4%BD%A9%E4%BD%A9%E8%B1%AC/peppa_pig_1.jpg" alt="peppa_pig" title="balloon twisting peppa_pig_1"></p><span id="more"></span><p>參考Ms Balloon Anna小姐的教學做的~ <a href="https://www.youtube.com/watch?v=hXmbpNlSUi8">教學影片傳送門</a><br>材料:</p><ul><li>身體:紅色260氣球，1&#x2F;3條即可</li><li>臉:粉紅色260氣球，充氣至尾端留8指</li><li>剪刀</li><li>黑色奇異筆</li><li>紅色奇異筆</li><li>粉紅色奇異筆(或是豆豆標籤)</li><li>白色油漆筆(或是豆豆標籤)</li></ul><p>結果好像兔子，沒關係，貼出來作為失敗例子供大家參考。<br>其實Ms Balloon Anna小姐的步驟很清楚，<br>是我畫工不佳，<br>應該要用粉紅色畫輪廓、有眼白，<br>再多揣摩五官位置才會像佩佩豬一些。</p><p>因為我自己容易倒帶之後反而忘記我目前做到哪一個步驟,<br>所以參考台大氣球社一位學長提出的記譜方式製作這張流程圖<br><img src="/2020/05/04/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E4%BD%A9%E4%BD%A9%E8%B1%AC/g.jpg" alt="flow" title="balloon twisting flow"><br>氣球譜 <a href="https://mropengate.blogspot.com/2016/03/blog-post_87.html">原文</a><br>邊上的文字是表示 – 第幾個步驟:氣泡長度(單位是自己的手指頭寬度）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2020/05/04/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E4%BD%A9%E4%BD%A9%E8%B1%AC/peppa_pig_1.jpg&quot; alt=&quot;peppa_pig&quot; title=&quot;balloon twisting peppa_pig_1&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Balloon Twisting" scheme="https://annsonic.github.io/categories/Balloon-Twisting/"/>
    
    
    <category term="Peppa Pig" scheme="https://annsonic.github.io/tags/Peppa-Pig/"/>
    
  </entry>
  
  <entry>
    <title>造型氣球-小紅帽</title>
    <link href="https://annsonic.github.io/2020/03/28/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%B0%8F%E7%B4%85%E5%B8%BD/"/>
    <id>https://annsonic.github.io/2020/03/28/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%B0%8F%E7%B4%85%E5%B8%BD/</id>
    <published>2020-03-28T02:09:16.000Z</published>
    <updated>2020-08-31T04:57:59.826Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2020/03/28/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%B0%8F%E7%B4%85%E5%B8%BD/red_riding_hood.jpg" alt="red_riding_hood" title="balloon twisting red_riding_hood"></p><span id="more"></span><p>參考Smile for Kids先生的教學做的~ <a href="https://www.youtube.com/watch?v=dM6hwEHLDjM">教學影片傳送門</a><br>材料:</p><ul><li>斗篷:紅色260氣球，充氣至尾端留6指</li><li>臉:膚色260氣球，用半條氣球長度即可</li><li>頭髮:褐色260氣球 (黃色、藍色、黑色也不錯看)，用半條氣球長度即可</li><li>剪刀</li><li>黑色奇異筆</li><li>紅色奇異筆(或是豆豆標籤)</li></ul><p>造型看起來很簡單，<br>沒想到斗篷的4個連續熊耳結就讓我弄爆氣球了，<br>斗篷帽子的長度我抓得太長了，<br>不該算到下巴，<br>教學影片只算到臉頰的長度，<br>感覺上是9個指頭的長度，<br>事後補救，於是我塞了一小截紅色氣球來補空隙，<br>啊…別太完美主義，<br>我的成品也很可愛對吧？</p><p>因為我自己容易倒帶之後反而忘記我目前做到哪一個步驟,<br>所以參考台大氣球社一位學長提出的記譜方式製作這張流程圖<br><img src="/2020/03/28/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%B0%8F%E7%B4%85%E5%B8%BD/g.jpg" alt="flow" title="balloon twisting flow"><br>氣球譜 <a href="https://mropengate.blogspot.com/2016/03/blog-post_87.html">原文</a><br>邊上的文字是表示 – 第幾個步驟:氣泡長度(單位是自己的手指頭寬度）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2020/03/28/%E9%80%A0%E5%9E%8B%E6%B0%A3%E7%90%83-%E5%B0%8F%E7%B4%85%E5%B8%BD/red_riding_hood.jpg&quot; alt=&quot;red_riding_hood&quot; title=&quot;balloon twisting red_riding_hood&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Balloon Twisting" scheme="https://annsonic.github.io/categories/Balloon-Twisting/"/>
    
    
    <category term="Little Red Riding Hood" scheme="https://annsonic.github.io/tags/Little-Red-Riding-Hood/"/>
    
  </entry>
  
  <entry>
    <title>利用YouTube Data API爬數據-分析大賣場影片</title>
    <link href="https://annsonic.github.io/2020/03/26/%E5%88%A9%E7%94%A8YouTube-Data-API%E7%88%AC%E6%95%B8%E6%93%9A-%E5%88%86%E6%9E%90%E5%A4%A7%E8%B3%A3%E5%A0%B4%E5%BD%B1%E7%89%87/"/>
    <id>https://annsonic.github.io/2020/03/26/%E5%88%A9%E7%94%A8YouTube-Data-API%E7%88%AC%E6%95%B8%E6%93%9A-%E5%88%86%E6%9E%90%E5%A4%A7%E8%B3%A3%E5%A0%B4%E5%BD%B1%E7%89%87/</id>
    <published>2020-03-26T13:05:15.000Z</published>
    <updated>2020-08-31T09:59:44.562Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2020/03/26/%E5%88%A9%E7%94%A8YouTube-Data-API%E7%88%AC%E6%95%B8%E6%93%9A-%E5%88%86%E6%9E%90%E5%A4%A7%E8%B3%A3%E5%A0%B4%E5%BD%B1%E7%89%87/bar.png" alt="Average number of comments by mart band" title="avg comments"></p><span id="more"></span><h2 id="主題："><a href="#主題：" class="headerlink" title="主題："></a>主題：</h2><p>針對大賣場相關的商品體驗影片，驗證台灣觀眾會因為賣場品牌而有不同的關注點，關注點分為3類：好物推薦、新品開箱、其他消費體驗</p><h2 id="假設："><a href="#假設：" class="headerlink" title="假設："></a>假設：</h2><p>(1)影片的留言者皆非大賣場員工，留言數正比於觀眾的共鳴強度</p><p>(2)關鍵字可以反映商業的意圖，故可將影片的意圖屬性簡單分類為好物推薦、新品開箱、其他消費體驗，假設我選定的關鍵字具有代表性</p><p>(3)YouTube回傳的搜尋結果(前50部片)具有取樣代表性</p><h2 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h2><p>(1)搜尋標題帶有大賣場名稱的影片，亦即：全聯、家樂福、好市多 (排除大潤發、愛買與頂好，因為影片數量過少)</p><p>(2)將影片標題依關鍵字分為3大類：好物推薦(Promoted)、新品開箱(Trial)、其他消費體驗(Other)，對應關鍵字為</p><ul><li>好物推薦 - 必買、CP值、推薦、評比</li><li>新品開箱 - 開箱、介紹、試<br>而標題不帶上述關鍵字者歸入其他消費體驗類</li></ul><p>(3)以文字雲分別檢視各類的影片標題、影片留言的熱門字</p><p>(4)統計三類影片的留言數</p><h2 id="實驗限制："><a href="#實驗限制：" class="headerlink" title="實驗限制："></a>實驗限制：</h2><p>受限個人不熟悉自然語言處理，所以能對文字雲做的語意分析程度有限</p><h2 id="結果"><a href="#結果" class="headerlink" title="結果:"></a>結果:</h2><p><img src="/2020/03/26/%E5%88%A9%E7%94%A8YouTube-Data-API%E7%88%AC%E6%95%B8%E6%93%9A-%E5%88%86%E6%9E%90%E5%A4%A7%E8%B3%A3%E5%A0%B4%E5%BD%B1%E7%89%87/bar.png" alt="Average number of comments by mart band" title="avg comments"></p><p>(1)條狀圖顯示台灣觀眾會因為賣場品牌而有不同的關注點，關注點分為3類：好物推薦(blue column)、新品開箱(orange column)、其他消費體驗(green column)。</p><p>(2)好市多的開箱類影片最吸引觀眾留言，一部開箱影片的平均留言人數達419人次、遠高於好物推薦類(128人次)與其他消費體驗類(79人次)，顯著不同於全聯和家樂福的情況，推測因為好市多的商品是大份量販售、平均單價低廉但總價高，故消費者在購買前會比較謹慎，會參考別人的開箱心得再決定購買，已購買者也會樂意分享個人心得於留言。合計平均留言人數也是勝過其他兩個賣場。</p><p>(3)文字雲顯示，我的關鍵字分類還是不夠精確，新品開箱類影片被歸類為其他消費體驗類，例如全聯的抹茶；收集的留言數量也隨著不同時間點搜尋影片而得到不同的數字。不過普遍來說，開箱類影片比起推薦類影片有更高的留言量，推測是因為開箱類傾向於介紹新檔期美食，觀眾好奇美食的口味、也易引起已嘗鮮者的共鳴。</p><ul><li>全聯其他消費體驗類標題<br>  <img src="/2020/03/26/%E5%88%A9%E7%94%A8YouTube-Data-API%E7%88%AC%E6%95%B8%E6%93%9A-%E5%88%86%E6%9E%90%E5%A4%A7%E8%B3%A3%E5%A0%B4%E5%BD%B1%E7%89%87/pxmart_other_title.png" alt="Word_cloud_pxmart_other_title" title="word cloud"></li></ul><p>(4)全聯影片的標題文字雲集中在強打甜點活動的品項上，感覺跟全聯注重社群行銷有關，業配影片的嫌疑性高。</p><ul><li>好物標題文字雲(全聯 vs 家樂福)<br>  <img src="/2020/03/26/%E5%88%A9%E7%94%A8YouTube-Data-API%E7%88%AC%E6%95%B8%E6%93%9A-%E5%88%86%E6%9E%90%E5%A4%A7%E8%B3%A3%E5%A0%B4%E5%BD%B1%E7%89%87/pxmart_promote_title.png" alt="Word_cloud_pxmart_promote_title" title="word cloud"><br>  <img src="/2020/03/26/%E5%88%A9%E7%94%A8YouTube-Data-API%E7%88%AC%E6%95%B8%E6%93%9A-%E5%88%86%E6%9E%90%E5%A4%A7%E8%B3%A3%E5%A0%B4%E5%BD%B1%E7%89%87/carrefour_promote_title.png" alt="Word_cloud_carrefour_promote_title" title="word cloud"></li></ul><hr><h2 id="後記"><a href="#後記" class="headerlink" title="後記"></a>後記</h2><p>(非教學文)這學期的作業。<br>老師出題的用意只是讓學生練習使用YouTube Data API爬影片的數據，<br>不過因為要分析的主題是自己決定，<br>反而花了很多時間想研究題目，<br>分析結果不是很專業，<br>但被選為當次模範作業，<br>頗有成就感所以紀錄一下以茲紀念。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2020/03/26/%E5%88%A9%E7%94%A8YouTube-Data-API%E7%88%AC%E6%95%B8%E6%93%9A-%E5%88%86%E6%9E%90%E5%A4%A7%E8%B3%A3%E5%A0%B4%E5%BD%B1%E7%89%87/bar.png&quot; alt=&quot;Average number of comments by mart band&quot; title=&quot;avg comments&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Project" scheme="https://annsonic.github.io/categories/Project/"/>
    
    
    <category term="YouTube Data API" scheme="https://annsonic.github.io/tags/YouTube-Data-API/"/>
    
  </entry>
  
</feed>
